---
title: "homework-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Quesion 1**
Describe the null hypotheses to which the p-values given in Table 3.4
correspond. Explain what conclusions you can draw based on these
p-values. Your explanation should be phrased in terms of sales, TV,
radio, and newspaper, rather than in terms of the coefficients of the
linear model.
**The null hypothesis states that there is no relationship between the X and Y variables. In this case its spending money on TV, radio, and newspaper advertising (X) does not increase sales (Y). For TV and radio adverstising, the p-value is < 0.0001, meaning we should reject the null hypothesis. For a given amount of TV and radio adverstising, spending an additional $1,000 on advertising leads to an increase of 46 and 189 units sold for TV's and radios respectively. In the case of newspaper advertising, the p-value is 0.8599, which indicates strong evidence for the null hypothesis, so we reject the alternative hypothesis. Based on these p-values, radio advertising seems to be the most effective so investing a budget in that would be the most efficient strategy.**

**question 3**
Suppose we have a data set with five predictors, X1 =GPA, X2 = IQ,
X3 = Gender (1 for Female and 0 forMale), X4 = Interaction between
GPA and IQ, and X5 = Interaction between GPA and Gender. The
response is starting salary after graduation (in thousands of dollars).
Suppose we use least squares to fit the model, and get
ˆβ0 = 50, ˆβ1 = 20, ˆβ2 = 0.07, ˆβ3 = 35, ˆβ4 = 0.01, ˆβ5 = −10.

(a) Which answer is correct, and why?

  i. For a fixed value of IQ and GPA, males earn more on average than females.
  
  ii. For a fixed value of IQ and GPA, females earn more on average than males.
  
  iii. For a fixed value of IQ and GPA, males earn more on average 
  than females provided that the GPA is high enough.
  
  iv. For a fixed value of IQ and GPA, females earn more on
  average than males provided that the GPA is high enough.
  
  **The least square line is yhat = 50 +20GPA + 0.07IQ + 35Gender + 0.01GPA X IQ - 10GPA X Gender. For males, it becomes yhat = 50 +20GPA + 0.07IQ + 0.01GPA X IQ and for females its yhat = 85 + 10GPA +0.07IQ + 0.01GPA X IQ. From these equations for males and females, we can see that the starting salary for males is higher than for females on average iff 50 + 20GPA >= 85 + 10GPA which is the same as GPA >= 3.5. Therefore the third(iii) option is correct.**
  
(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.
**We can plug in the given values in the equation for females and get yhat = 85 + 40 + 7.7 + 4.4 = 137.1, so the starting salary would be $137100.**

(c) True or false: Since the coefficient for the GPA/IQ interaction
term is very small, there is very little evidence of an interaction
effect. Justify your answer.
**In order to test if GPA/IQ has an interaction effect we would have to test the null hypothesis that β4^=0 and look at the p-value. So the answer is false.**

**question 4**
I collect a set of data (n = 100 observations) containing a single
predictor and a quantitative response. I then fit a linear regression
model to the data, as well as a separate cubic regression, i.e. Y =
β0 + β1X + β2X2 + β3X3 + .

(a) Suppose that the true relationship between X and Y is linear,
i.e. Y = β0 + β1X + . Consider the training residual sum of
squares (RSS) for the linear regression, and also the training
RSS for the cubic regression. Would we expect one to be lower
than the other, would we expect them to be the same, or is there
not enough information to tell? Justify your answer.
**Because the relationship between X and Y is linear, we can expect that the least squares line to be close to the true regression line, and this the RSS for the linear regression may be lower than for the cubic regression.**
(b) Answer (a) using test rather than training RSS.
**In this case the test RSS depends on the test data, so not enough information is there to conclude.**

(c) Suppose that the true relationship between X and Y is not linear,
but we don’t know how far it is from linear. Consider the training
RSS for the linear regression, and also the training RSS for the
cubic regression. Would we expect one to be lower than the
other, would we expect them to be the same, or is there not
enough information to tell? Justify your answer.
**Polynomial regression has lower train RSS than the linear fit because of its higher flexibility thus no matter what the underlying relationship is the more flexible model will closer follow points and reduce train RSS.**

(d) Answer (c) using test rather than training RSS.
**Again, not enough information is given to tell which test RSS would be lower for either regression. If it is closer to linear than ubic, the linear regression RSS could be lower than the cubic regression test RSS. If it is closer to cubic than linear, the cubix rregression test RSS could be lower thean the linear regression test RSS.**

**Question 15**
This problem involves the Boston data set, which we saw in the lab
for this chapter. We will now try to predict per capita crime rate
using the other variables in this data set. In other words, per capita
crime rate is the response, and the other variables are the predictors.
```{r}
library(MASS)
attach(Boston)
fit.zn <- lm(crim ~zn)
summary(fit.zn)
```
(a) For each predictor, fit a simple linear regression model to predict
the response. Describe your results. In which of the models is
there a statistically significant association between the predictor
and the response? Create some plots to back up your assertions.
```{r}
fit.indus <- lm(crim ~ indus)
summary(fit.indus)
```

```{r}
chas <- as.factor(chas)
fit.chas <- lm(crim ~ chas)
summary(fit.chas)
```

```{r}
fit.nox <- lm(crim ~ nox)
summary(fit.nox)
```

```{r}
fit.rm <- lm(crim ~ rm)
summary(fit.rm)
```

```{r}
fit.age <- lm(crim ~ age)
summary(fit.age)
```

```{r}
fit.dis <- lm(crim ~ dis)
summary(fit.dis)
```

```{r}
fit.rad <- lm(crim ~ rad)
summary(fit.rad)
```

```{r}
fit.tax <- lm(crim ~ tax)
summary(fit.tax)
```

```{r}
fit.ptratio <- lm(crim ~ ptratio)
summary(fit.ptratio)
```

```{r}
fit.black <- lm(crim ~ black)
summary(fit.black)
```

```{r}
fit.lstat <- lm(crim ~ lstat)
summary(fit.lstat)
```

```{r}
fit.medv <- lm(crim ~ medv)
summary(fit.medv)
```

(b) Fit a multiple regression model to predict the response using
all of the predictors. Describe your results. For which predictors
can we reject the null hypothesis H0 : βj = 0?
```{r}
fit.all <- lm(crim ~ ., data = Boston)
summary(fit.all)
```
**We reject the null hypothesis for zn, dis, rad, black, and medv.**

(c) How do your results from (a) compare to your results from (b)?
Create a plot displaying the univariate regression coefficients
from (a) on the x-axis, and the multiple regression coefficients
from (b) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coefficient in a simple linear regression
model is shown on the x-axis, and its coefficient estimate
in the multiple linear regression model is shown on the y-axis.
```{r}
simple.reg <- vector("numeric",0)
simple.reg <- c(simple.reg, fit.zn$coefficient[2])
simple.reg <- c(simple.reg, fit.indus$coefficient[2])
simple.reg <- c(simple.reg, fit.chas$coefficient[2])
simple.reg <- c(simple.reg, fit.nox$coefficient[2])
simple.reg <- c(simple.reg, fit.rm$coefficient[2])
simple.reg <- c(simple.reg, fit.age$coefficient[2])
simple.reg <- c(simple.reg, fit.dis$coefficient[2])
simple.reg <- c(simple.reg, fit.rad$coefficient[2])
simple.reg <- c(simple.reg, fit.tax$coefficient[2])
simple.reg <- c(simple.reg, fit.ptratio$coefficient[2])
simple.reg <- c(simple.reg, fit.black$coefficient[2])
simple.reg <- c(simple.reg, fit.lstat$coefficient[2])
simple.reg <- c(simple.reg, fit.medv$coefficient[2])
mult.reg <- vector("numeric", 0)
mult.reg <- c(mult.reg, fit.all$coefficients)
mult.reg <- mult.reg[-1]
plot(simple.reg, mult.reg, col = "red")
```
There is a difference between the simple and multiple regression coefficients, due to the fact that in simple regression the slope represents the average effect of an increase in the predictor where multiple regression's slope represents the average effect of an increase in the predictor while holding other predictors fixed.

(d) Is there evidence of non-linear association between any of the
predictors and the response? To answer this question, for each
predictor X, fit a model of the form
Y = β0 + β1X + β2X2 + β3X3 + .

```{r}
fit.zn2 <- lm(crim ~ poly(zn, 3))
summary(fit.zn2)
```

```{r}
fit.indus2 <- lm(crim ~ poly(indus, 3))
summary(fit.indus2)
```

```{r}
fit.nox2 <- lm(crim ~ poly(nox, 3))
summary(fit.nox2)
```

```{r}
fit.rm2 <- lm(crim ~ poly(rm, 3))
summary(fit.rm2)

```

```{r}
fit.age2 <- lm(crim ~ poly(age, 3))
summary(fit.age2)
```

```{r}
fit.dis2 <- lm(crim ~ poly(dis, 3))
summary(fit.dis2)
```

```{r}
fit.rad2 <- lm(crim ~ poly(rad, 3))
summary(fit.rad2)
```

```{r}
fit.tax2 <- lm(crim ~ poly(tax, 3))
summary(fit.tax2)
```

```{r}
fit.ptratio2 <- lm(crim ~ poly(ptratio, 3))
summary(fit.ptratio2)
```

```{r}
fit.black2 <- lm(crim ~ poly(black, 3))
summary(fit.black2)
```

```{r}
fit.lstat2 <- lm(crim ~ poly(lstat, 3))
summary(fit.lstat2)
```

```{r}
fit.medv2 <- lm(crim ~ poly(medv, 3))
summary(fit.medv2)
```






