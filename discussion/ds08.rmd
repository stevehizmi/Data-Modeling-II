---
author: Steven Hizmi
---


---
```{r, message=FALSE}
library(tidyverse)
library(dbplyr)
```

1) Suppose that you flip a coin 2000 times and it comes up heads in 1887 of the tosses. Compute a 95% confidence interval for the true probability of heads.

```{r, message = FALSE}
n = 2000
n_heads = 1887

phat = n_heads / n
phat

# rbinom size = 1 is same as bernoulli

simulate_S = function(x){
  X = rbinom(n, 1, phat)
  S = mean(X)
  return(S)
}

r = 10000
monte_carlo = data.frame(replicate = 1:r,
                         S = rep(NA,r))

for(i in 1:r){
  monte_carlo$S[i] = simulate_S(phat)
}

monte_carlo = as.tibble(monte_carlo)
qs = monte_carlo$S %>% quantile(prob = c(0.025, 0.975))
qs
```




2)Suppose that the true probability is p = 1887/2000. 

a)Simulate one experiment. Then, compute a confidence interval for p. 
b)Does it cover it? Write code to check this.

```{r}
p_true = 1887/ 2000
data = rbinom(1,  size = 2000, p_true )

simulate_S = function(data){
  phat = data/2000
  X = rbernoulli(n = 2000, phat)
  S = mean(X)
  return(S)
}

compute_ci = function(data){
  r = 1000
  monte_carlo = data.frame(replicate = 1:r,
                           S = rep(NA,r))
  for(i in 1:r){
    monte_carlo$S[i] = simulate_S(data)
  }
  monte_carlo = as.tibble(monte_carlo)
  qs = monte_carlo$S %>% quantile(prob = c(0.025, 0.975))
  return(qs)
}
qs = compute_ci(data)
qs

# does it cover it? (qs[1] <= p_true) && (p_True <= qs[2])
(qs[1] <= p_true) && (p_true <= qs[2])

```



3)We will now investigate the coverage properties of this 95% confidence interval. Put your code from 2a into simulate_S. Put your code from 2b into check_if_S_in_A. (you are going to have two “simulate_S” functions, but they will need different names!)
```{r}
simulate_ci = function(){
  data = rbinom(1, size = 2000, 1887/2000)
  
  interval = compute_ci(data)
  
  return(interval)
}

check_if_X_in_A = function(lower, upper, true_p){
  return((lower <= p_true) && (p_true <= upper))
}

r = 100
monte_carlo = data.frame(replicate = 1:r,
                         X1 = rep(NA,r),
                         X2 = rep(NA,r),
                         X_in_A = rep(NA,r))
for(i in 1:r){
  simulated_interval = simulate_ci()
  monte_carlo$X1[i] = simulated_interval[1]
  monte_carlo$X2[i] = simulated_interval[2]
  monte_carlo$X_in_A[i] = check_if_X_in_A(monte_carlo$X1[i],monte_carlo$X2[i], 1887/2000)
}

monte_carlo = as.tibble(monte_carlo)
monte_carlo %>% summarize(mean(X_in_A))
```

answer: 95% percent of the ci will cover the true probability



4)What proportion of times does your 95% confidence interval cover the true parameter? Compute with Monte Carlo. 
A)
B)
C)
```{r}
# naive confidence interval
confint.recapture.exact = function(num.1st,num.2nd,num.2nd.tag,level = 0.95){
  k = num.2nd.tag
  K = num.2nd
  n = num.1st
  a2 = (1-level)/2
  k5 = k+0.5
  nk5 = n-k+0.5
  Kk5 = K-k+0.5
  sig = sqrt(1/k5+1/Kk5+1/nk5+k5/(nk5*Kk5))
  int = K+nk5-1+Kk5*nk5/k5*exp(c(1,-1)*qnorm(a2)*sig)
  names(int) = stats:::format.perc(c(a2,1-a2),3)
  return(int)
}

confint.recapture.exact(501, 502, 27)

# compute estimate of N
N_est = 501*502/8


# method 1: use rbinom
sim_tag = function(){
  tagged = rbinom(1,size=502,p=501/N_est)
  return(tagged)
}

# method 2: use sample
sim_tag2 = function(){
  june_fish = sample.int(round(N_est),size=501)
  july_fish = sample.int(round(N_est),size=502)
  tagged = length(base::intersect(june_fish,july_fish))
  return(tagged)
}

# for each rep, compute naive interval for N and
# check if it contains our original N_est
check_naive_covered = function(num_tag){
  int = confint.recapture.exact(501,502,num_tag)
  if(N_est >= int[1] && N_est <= int[2]){
    return(TRUE)
  } else return(FALSE)
}

#' Now, we run both methods and compare their coverage.

r = 10000
res = rep(NA,r)
for(i in 1:r){
  res[i] = check_naive_covered(sim_tag())
}
mean(res)

res2 = rep(NA,r)
for(i in 1:r){
  res2[i] = check_naive_covered(sim_tag2())
}
mean(res2)

#' Their coverage seem to be pretty similar. Just for fun, let's directly plot
#' the output of the two simulation methods for comparison.

#+ message=F,warning=F,cache=T
library(tidyverse)
r = 100000
data.frame(method=gl(2,r,labels=c("binom","sample")),
           tagged=c(replicate(r,sim_tag()),replicate(r,sim_tag2()))) %>%
  ggplot(aes(x=tagged)) + facet_wrap(vars(method),ncol=1) + geom_bar()

#' The two methods give nearly identical results, so we don't need to
#' worry about which method we are using.

# define function for checking coverage of a single MC interval
single.MC.covered = function(r,level=.95,num1=501,num2=502,tagged=8){
  
  # convert confidence level to half-area on either side
  # e.g. if level is 0.95, a2 is 0.025
  a2 = (1-level)/2
  
  # get "true" value of N
  N.est = num1*num2/tagged
  
  # define function to simulate number of tagged fish using rbinom,
  # then using that to compute a new simulated estimate of N
  sim.N = function(N) num1*num2/rbinom(1,size=num1,p=num1/N)
  
  # run sim.N to get 1 experiment where we assume the true value of N is
  # N.est, and we go out in July and catch 502 fish, and use our tagged count
  # to produce a single estimate of what we THINK N is
  # (note in this experiment, we don't know the real N is N.est, we just know
  # we THINK N is roughly N.sim)
  N.sim = sim.N(N.est)
  
  # using our N.sim, we RERUN sim.N r times to get a MC confidence interval
  # around out N.sim, using a2 and 1-a2 as the lower and upper quantiles
  ci = quantile(replicate(r,sim.N(N.sim)),c(a2,1-a2))
  
  # return TRUE if CI contains the true value N.est, or FALSE otherwise
  return(N.est>=ci[1] && N.est<=ci[2])
}

#' Hopefully that wasn't too confusing. Let's check the coverage of this
#' MC interval. For fun, let's again do a low rep and high rep and compare them.
#' Note the difference below between `r` and `n`.
#' `r` is the **number of reps used to construct** the MC interval, 
#' whereas `n` is the **number of times we test these intervals**.
#' For low/high reps, we let `r=100` vs `r=1000` to see if using more reps gives
#' better coverage. To estimate this coverage, we use the same `n=1000` for both.

#+ cache=T
# get coverage of low r
mean(
  replicate(n=1000,single.MC.covered(r=100))
)

# get coverage of high r
mean(
  replicate(n=1000,single.MC.covered(r=1000))
)

#' It seems like the high `r` intervals are more reliable. If we are **REALLY**
#' crazy, we can go _**ONE LEVEL DEEPER**_ and repeat the above chunk many times
#' to see how variable the coverage of each interval is. This is getting super meta,
#' since we're using MC to determine the variability of the MC-estimated coverage
#' of an MC-computed interval. This is starting to reach the computational limits of
#' my single machine, so I'm just setting `n=200` in the coverage computation and
#' only use 200 coverage computations to find the variability, but it
#' should be enough to give us an idea of the variability of each coverage.
#' 
#' I'm also going to throw in the naive interval for comparison, so we can see
#' whether or not it's on par with the MC intervals.

#+ cache=T
df.var = data.frame(
  method = gl(3,200,labels=c("low r MC","high r MC",'naive')),
  coverage = c(
    # get low-r coverages
    replicate(
      200,
      mean(
        replicate(n=200,single.MC.covered(r=100))
      )
    ), 
    # get high-r coverages
    replicate(
      200,
      mean(
        replicate(n=200,single.MC.covered(r=1000))
      )
    ),
    replicate(
      200,
      mean(
        replicate(n=200,check_naive_covered(sim_tag()))
      )
    )
  )
)
ggplot(df.var,aes(x=coverage)) + facet_wrap(vars(method),ncol=1) + geom_bar() +
  scale_x_continuous(expand=c(.02,0),
                     breaks=seq(min(df.var$coverage),max(df.var$coverage),by=.01))

#' As you can see, the low-r 95% MC confidence interval sometimes achieves
#' the desired level of 95% coverage, but it often falls short. However,
#' a high-r 95% MC confidence interval much more often achieves the desired
#' 95% coverage.
#' 
#' Surprisingly, the naive interval is able to achieve even better coverage
#' than both the MC methods. We can quickly compute what proportion of the resulting
#' coverages are at least 95%.

#+ cache=T
df.var %>% 
  group_by(method) %>% 
  summarize(">= 0.95" = mean(coverage >= 0.95), .groups="drop")

#' Our results tell us that roughly speaking, for this problem,
#' low-r MC intervals give us about 19% chance of having >= 95% coverage,
#' high-r MC intervals give us about 58% chance of having >= 95% coverage, and
#' naive intervals give us about 91% chance of having >= 95% coverage.
```


On what proportion of experiments does the confidence interval cover the truth? 

check last output of q4

